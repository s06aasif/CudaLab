{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b1fc6a-fb67-4752-951c-1c7c8586703d",
   "metadata": {},
   "source": [
    "Group Members:\n",
    "1.Abhishek S Pillai\n",
    "2.Areesha Asif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa5a89-d5aa-4785-b2bf-5da7e3a9fbaf",
   "metadata": {},
   "source": [
    "Disclaimer:-\n",
    "\n",
    "We had some trouble in implementing the code.In the task2 part, initially we have tried to worked on the processed image part but due to issues with loading the data, we have to abort all the methodlogy used. So we currently have taken a cue from other collegues on how to upload the data for the 2nd part.\n",
    "\n",
    "Also in task 2, especially at the LSTM implementation part, we constanly faced the problem of out of memory and Tensor Mismatch issues which prevented us from executing the remaining part of the question and seeing the accuracy and losses. We worked extensively to remove these errors with the help of chatgpt,github,etc but couldnt resolve it due to time constrains.\n",
    "\n",
    "However we have understood the demand of the question and tried to write the code of all questions been asked.\n",
    "We have shied away from plotting graphs due to unavailabilty of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a25bc-a2b4-404a-a21a-0c097fb5f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a182-9183-4b12-b107-e02d8c515b2e",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Implement a LSTM (LSTM() and/or LSTMCell()) from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266b256-1804-419e-82d2-55a286c60eb1",
   "metadata": {},
   "source": [
    "Here were implementing the LSTM from scratch.we are just using the MNIST data to check the implemnation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed2f0ea-0a35-4e4a-b1b0-81ef09be6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and Loading Dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download=True)\n",
    " \n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3d07d8-de9c-442c-bad4-9cc31f26dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting data loaders for iterating\n",
    "B_SIZE = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=B_SIZE, \n",
    "                                           shuffle=True) \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=B_SIZE,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9373b14-4b1e-4590-91c6-9d7732ab2350",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac51921-cd31-46d1-a0ad-906c8bf21758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1, mode=\"zeros\", dropout_prob=0.5):\n",
    "        assert mode in [\"zeros\", \"random\", \"learned\"]\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"learned\":\n",
    "            self.learned_h = nn.Parameter(torch.randn(num_layers, 1, hidden_dim).requires_grad_())\n",
    "            self.learned_c = nn.Parameter(torch.randn(num_layers, 1, hidden_dim).requires_grad_())\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=emb_dim),\n",
    "            nn.ReLU()  # Adding ReLU activation\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob if num_layers > 1 else 0  # Applying dropout if there are multiple layers\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_dim, out_features=10),\n",
    "            nn.Softmax(dim=1)  # Adding softmax activation for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b_size, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device)\n",
    "\n",
    "        x_rowed = x.view(b_size, n_channels * n_rows, n_cols)\n",
    "        embeddings = self.encoder(x_rowed)\n",
    "\n",
    "        lstm_out, (h_out, c_out) = self.lstm(embeddings, (h, c))\n",
    "\n",
    "        y = self.classifier(lstm_out[:, -1, :])\n",
    "        return y\n",
    "\n",
    "    def init_state(self, b_size, device):\n",
    "        if self.mode == \"zeros\":\n",
    "            h = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif self.mode == \"random\":\n",
    "            h = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif self.mode == \"learned\":\n",
    "            h = self.learned_h.repeat(1, b_size, 1)\n",
    "            c = self.learned_c.repeat(1, b_size, 1)\n",
    "        h = h.to(device)\n",
    "        c = c.to(device)\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88d976-b2f5-439d-be97-2786ae65ae8b",
   "metadata": {},
   "source": [
    "### LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8166e97-1e58-4fac-8be8-7ded3c68e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCells(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=2, mode=\"zeros\"):\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super(LSTMCells, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "\n",
    "        # Linear layer for embedding rows into vector representations\n",
    "        self.encoder = nn.Linear(in_features=input_dim, out_features=emb_dim)\n",
    "\n",
    "        # LSTM model\n",
    "        lstms = []\n",
    "        for i in range(num_layers):\n",
    "            in_size = emb_dim if i == 0 else hidden_dim\n",
    "            lstms.append(nn.LSTMCell(input_size=in_size, hidden_size=hidden_dim))\n",
    "        self.lstm = nn.ModuleList(lstms)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.classifier = nn.Linear(in_features=hidden_dim, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b_size, n_channels, n_rows, n_cols = x.shape\n",
    "        h, c = self.init_state(b_size=b_size, device=x.device)\n",
    "\n",
    "        # Embedding rows\n",
    "        x_rowed = x.view(b_size, n_channels * n_rows, n_cols)\n",
    "        embeddings = self.encoder(x_rowed)\n",
    "\n",
    "        # Iterating over sequence length\n",
    "        lstm_out = []\n",
    "        for i in range(embeddings.shape[1]):\n",
    "            lstm_input = embeddings[:, i, :]\n",
    "            # Iterating over LSTM Cells\n",
    "            for j, lstm_cell in enumerate(self.lstm):\n",
    "                h[j], c[j] = lstm_cell(lstm_input, (h[j], c[j]))\n",
    "                lstm_input = h[j]\n",
    "            lstm_out.append(lstm_input)\n",
    "        lstm_out = torch.stack(lstm_out, dim=1)\n",
    "\n",
    "        # Classifying\n",
    "        y = self.classifier(lstm_out[:, -1, :])  # Feeding only output at the last layer\n",
    "        return y\n",
    "\n",
    "    def init_state(self, b_size, device):\n",
    "        if self.mode == \"zeros\":\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        elif self.mode == \"random\":\n",
    "            h = [torch.randn(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.randn(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb907ce5-9e58-4f55-9a5b-29aad1693ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54c3c87-74d8-4857-8ce5-ac8530b15cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "         \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    for images, labels in eval_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(images)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    loss_iters = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        accuracy, loss = eval_model(\n",
    "                    model=model, eval_loader=valid_loader,\n",
    "                    criterion=criterion, device=device\n",
    "            )\n",
    "        valid_acc.append(accuracy)\n",
    "        val_loss.append(loss)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        scheduler.step()\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "        print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "        print(f\"    Accuracy: {accuracy}%\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, valid_acc\n",
    "\n",
    "\n",
    "def smooth(f, K=5):\n",
    "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
    "    kernel = np.ones(K) / K\n",
    "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
    "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
    "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
    "    return smooth_f\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ba8900-6858-427b-961e-1aa831f86d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db8c1d1a-f9ed-4882-919c-d28e234124f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234570"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = LSTM(input_dim=28, emb_dim=64, hidden_dim=128, num_layers=2, mode=\"zeros\")\n",
    "model = LSTMCells(input_dim=28, emb_dim=64, hidden_dim=128, num_layers=2, mode=\"zeros\") #do LSTM or LSTM Cell...\n",
    "count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22979ff9-4343-40e8-b119-5628254ee64d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1186b348-eea2-42f7-a907-3f978b8107dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ec58f85-b1f8-4e9a-91c6-0056a31babbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 235: loss 0.30209. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 32.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    Train loss: 1.12726\n",
      "    Valid loss: 2.30451\n",
      "    Accuracy: 9.74%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Iter 235: loss 0.29090. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "    Train loss: 0.27162\n",
      "    Valid loss: 0.35568\n",
      "    Accuracy: 89.31%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Iter 235: loss 0.13680. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 32.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "    Train loss: 0.17808\n",
      "    Valid loss: 0.18921\n",
      "    Accuracy: 94.44%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Iter 235: loss 0.27088. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 32.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "    Train loss: 0.13405\n",
      "    Valid loss: 0.15394\n",
      "    Accuracy: 95.39999999999999%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Iter 235: loss 0.11528. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 32.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "    Train loss: 0.11257\n",
      "    Valid loss: 0.11923\n",
      "    Accuracy: 96.36%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Iter 235: loss 0.02623. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 31.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "    Train loss: 0.08139\n",
      "    Valid loss: 0.09901\n",
      "    Accuracy: 97.03%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Iter 235: loss 0.13871. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 32.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "    Train loss: 0.07682\n",
      "    Valid loss: 0.08859\n",
      "    Accuracy: 97.36%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Iter 235: loss 0.06874. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 31.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "    Train loss: 0.07295\n",
      "    Valid loss: 0.08379\n",
      "    Accuracy: 97.44%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Iter 235: loss 0.04510. : 100%|███████████████████████████████████████████████| 235/235 [00:07<00:00, 31.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "    Train loss: 0.06931\n",
      "    Valid loss: 0.08281\n",
      "    Accuracy: 97.44%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Iter 235: loss 0.10867. : 100%|██████████████████████████████████████████████| 235/235 [00:07<00:00, 31.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "    Train loss: 0.06777\n",
      "    Valid loss: 0.07994\n",
      "    Accuracy: 97.57000000000001%\n",
      "\n",
      "\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "        train_loader=train_loader, valid_loader=test_loader, num_epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a34c4-718b-4808-9c0c-793ad5fe9f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d0404c-931d-404b-bdd7-ebafdbcfa47b",
   "metadata": {},
   "source": [
    "## Implement a Convolutional LSTM (ConvLSTM() and/or ConvLSTMCell()) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5083d9-0a84-453a-a579-878598f937c9",
   "metadata": {},
   "source": [
    "### Convolutional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cd1da4f4-6215-460a-b760-e286c7242b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv_i = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "        self.conv_f = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "        self.conv_o = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "        self.conv_g = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state, device):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        # Combine input and hidden states\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "\n",
    "        # Convolutional operations\n",
    "        combined_conv_i = self.conv_i(combined)\n",
    "        combined_conv_f = self.conv_f(combined)\n",
    "        combined_conv_o = self.conv_o(combined)\n",
    "        combined_conv_g = self.conv_g(combined)\n",
    "\n",
    "        # Split results to get input, forget, output, and cell gates\n",
    "        cc_i, cc_f, cc_o, cc_g = combined_conv_i, combined_conv_f, combined_conv_o, combined_conv_g\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size, device):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, c, h, w = input_tensor.size()\n",
    "        seq_len = 1\n",
    "\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w), device=input_tensor.device)\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        cur_layer_input = input_tensor.unsqueeze(1)\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, 0, :, :, :],\n",
    "                                                 cur_state=[h, c], device=self.cell_list[layer_idx].conv_i.weight.device)\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size, device):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size, device))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b7b9b01a-313f-4d20-bf87-139773fb2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "04a2fc50-916d-4235-b376-605ff4cd761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the ConvLSTM model: 449792\n"
     ]
    }
   ],
   "source": [
    "def count_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Example usage\n",
    "input_size = 3  # Number of input channels\n",
    "hidden_size = 64  # Number of hidden channels\n",
    "kernel_size = (3,3)  # Size of the convolutional kernel\n",
    "num_layers = 2  # Number of ConvLSTM layers\n",
    "\n",
    "model = ConvLSTM(input_size, hidden_size, kernel_size, num_layers)\n",
    "num_params = count_model_params(model)\n",
    "\n",
    "print(\"Number of parameters in the ConvLSTM model:\", num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "546d18c2-94cd-473a-92c0-12ee63b9d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1e372c76-347a-4ac3-8ce9-69624544ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "76a868b9-3c42-44ed-a121-7e8b0a6c1179",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss, loss_iters, valid_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 73\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     70\u001b[0m        \n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# validation epoch\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# important for dropout and batch norms\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     accuracy, loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     valid_acc\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     78\u001b[0m     val_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/CudaLab/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 44\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, eval_loader, criterion, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Forward pass only to get logits/output\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     47\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[163], line 100\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, input_tensor, hidden_state)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor, hidden_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[0;32m--> 100\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     b, c, h, w \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    103\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "        train_loader=train_loader, valid_loader=test_loader, num_epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42138b51-ba57-466e-b5a4-eddcfce7e2ec",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bc0ba-9a78-4339-b3a0-2884aa1f1726",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "### Perform \"Action Recognition\" on the KTH-Actions dataset:o## ns\r\n",
    "Use spatial dimensionality of frames of 64## x64\r\n",
    "Split videos into subsequences of e.g. 20 frames. Treat each of these subsequences as independ## ent.\r\n",
    "Feel free to use augmentations (temporal and/or spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a249f-82c6-4475-b637-946c9ccc37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2a15c-c026-433f-b929-13cb5d53f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc4917-fe06-477e-8513-e5e01c58d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Dataset/'\n",
    "actions = [name for name in os.listdir(path) if '.' not in name]\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb7bf8-18f2-418d-8d04-18da0f42c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_names, file_paths, labels = [], [], []\n",
    "# Iterate through each action category\n",
    "for i, action in enumerate(actions):\n",
    "    action_path = os.path.join(path, action)\n",
    "    names = os.listdir(action_path)\n",
    "    \n",
    "    # Collect file names, paths, and labels\n",
    "    file_names.extend(names)\n",
    "    file_paths.extend([os.path.join(action_path, name) for name in names])\n",
    "    labels.extend([i] * len(names))\n",
    "\n",
    "\n",
    "# Combine and sort data\n",
    "file_data = [[file_names[i], file_paths[i], labels[i]] for i in range(len(labels))]\n",
    "file_data.sort()\n",
    "\n",
    "# Now, file_data contains sorted video information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a7043-fea9-477a-afa4-700483c94f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = float('inf')\n",
    "\n",
    "for path in file_paths:\n",
    "    video, _, _ = read_video(path)\n",
    "    if video.shape[0] > 0 and video.shape[0] < min_length:\n",
    "        min_length = video.shape[0]\n",
    "\n",
    "if min_length == float('inf'):\n",
    "    print(\"No valid videos found.\")\n",
    "else:\n",
    "    print(f\"The shortest video has {min_length} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c141b-62f5-4bf1-bc95-ae3ba31f6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the point where we need to split the data\n",
    "file_data[430:432]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f6a79-bf8c-4f3b-bdf3-04e3ae03d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Custom Dataset Class\n",
    "##Split videos into subsequences of e.g. 20 frames.\n",
    "\n",
    "class CustomKTHDataset(Dataset):\n",
    "    def __init__(self, data_info, is_train=True, transform_ops=None, seq_len=20, frame_dim=(64, 64)):\n",
    "        self.is_train = is_train\n",
    "        self.seq_len = seq_len\n",
    "        self.frame_dim = frame_dim\n",
    "        self.transform_ops = transform_ops if transform_ops else transforms.Compose([transforms.ConvertImageDtype(torch.float)])\n",
    "        \n",
    "        # Split data into train/test\n",
    "        train_cutoff = int(len(data_info) * 0.8)  # 80-20 train-test split\n",
    "        self.data = data_info[:train_cutoff] if is_train else data_info[train_cutoff:]\n",
    "\n",
    "        self.video_paths = [item[1] for item in self.data]\n",
    "        self.labels = [torch.tensor(item[2], dtype=torch.long) for item in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label = self.labels[index]\n",
    "        try:\n",
    "            video_frames, _, _ = read_video(video_path, pts_unit='sec')\n",
    "            video_frames = self.process_frames(video_frames)\n",
    "        except Exception as error:\n",
    "            print(f\"Failed to load video {video_path}: {error}\")\n",
    "            video_frames = torch.zeros((self.seq_len, 3, *self.frame_dim))\n",
    "        return video_frames, label\n",
    "\n",
    "    def process_frames(self, frames):\n",
    "        processed_frames = [transforms.Resize(self.frame_dim)(frame) for frame in frames[:self.seq_len]]\n",
    "        if len(processed_frames) < self.seq_len:\n",
    "            padding = [torch.zeros((3, *self.frame_dim)) for _ in range(self.seq_len - len(processed_frames))]\n",
    "            processed_frames.extend(padding)\n",
    "        \n",
    "        video_tensor = torch.stack(processed_frames, dim=0).permute(0, 2, 3, 1)\n",
    "        if self.transform_ops:\n",
    "            video_tensor = self.transform_ops(video_tensor)\n",
    "        return video_tensor\n",
    "\n",
    "# Usage\n",
    "transformations = transforms.Compose([transforms.ConvertImageDtype(torch.float),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b0076-dd7d-44c6-9d1d-a4f2eff69508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are reducing the batch size to 2 because continiously we are getting the out of memory problem.\n",
    "\n",
    "train_dataset = CustomKTHDataset(file_data, is_train=True, transform_ops=transformations)\n",
    "test_dataset = CustomKTHDataset(file_data, is_train=False, transform_ops=transformations)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, drop_last=False)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2902ea-c267-46a8-917d-4af75ab56888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvolutionalEncoder is a new class that processes each frame through several convolutional\n",
    "#layers before flattening and passing it to a fully connected layer\n",
    "\n",
    "class ConvolutionalEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(ConvolutionalEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(64 * 8 * 8, output_size)  # Adjust the size according to your input dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66629cca-eb8d-4f56-a3c3-6ba88aac0786",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLSTM\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size: \u001b[38;5;28mint\u001b[39m, hidden_size: \u001b[38;5;28mint\u001b[39m, num_channels: \u001b[38;5;28mint\u001b[39m, frame_dim: \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_channels: int, frame_dim: tuple):\n",
    "        super().__init__()\n",
    "        self.input_size, self.hidden_size = input_size, hidden_size\n",
    "        self.frame_dim = frame_dim\n",
    "\n",
    "        # Add a convolutional layer to process the frames\n",
    "        self.conv_layer = nn.Conv2d(num_channels, input_size, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Convolutional encoder\n",
    "        self.conv_encoder = ConvolutionalEncoder(num_channels, input_size)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.layer = nn.Linear(input_size * frame_dim[0] * frame_dim[1] + hidden_size, hidden_size * 4)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _, _, _ = x.shape\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        c = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        outputs = []\n",
    "\n",
    "        for frame in x.transpose(0, 1):\n",
    "            # Process each frame with the convolutional layer\n",
    "            conv_out = self.conv_layer(frame)\n",
    "            conv_out = conv_out.view(batch_size, -1)\n",
    "\n",
    "            # LSTM computation\n",
    "            input = torch.cat((conv_out, h), dim=1)\n",
    "            forget_gate, input_gate, cell_gate, output_gate = self.layer(input).chunk(4, dim=1)\n",
    "            forget_gate = torch.sigmoid(forget_gate)\n",
    "            input_gate = torch.sigmoid(input_gate)\n",
    "            cell_gate = torch.tanh(cell_gate)\n",
    "            output_gate = torch.sigmoid(output_gate)\n",
    "            c = forget_gate * c + input_gate * cell_gate\n",
    "            h = output_gate * torch.tanh(c)\n",
    "            outputs.append(h)\n",
    "\n",
    "        return outputs, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ee62e-13a9-4b81-8fca-31713fc319fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels: int, hidden_channels: int, device: torch.device):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels, self.hidden_channels = input_channels, hidden_channels\n",
    "        self.device = device  # Device (CPU/GPU) on which to run the model\n",
    "\n",
    "        \n",
    "        # Convolutional LSTM layer\n",
    "        self.layer = nn.Conv2d(\n",
    "            input_channels + hidden_channels, hidden_channels * 4, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(hidden_channels * 4)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> 'tuple[list, tuple[torch.Tensor, torch.Tensor]]':\n",
    "        # Initialize hidden and cell states\n",
    "        h = torch.zeros(x.shape[0], self.hidden_channels,\n",
    "                        x.shape[-2], x.shape[-1], device=self.device)\n",
    "        c = torch.zeros(x.shape[0], self.hidden_channels,\n",
    "                        x.shape[-2], x.shape[-1], device=self.device)\n",
    "        outputs = []\n",
    "\n",
    "\n",
    "        \n",
    "        # Process each frame in the sequence\n",
    "        for frame in x.transpose(0, 1):  # Transpose to iterate over frames\n",
    "            combined = torch.cat((frame, h), dim=1)  # Combine frame and hidden state\n",
    "            gates = self.bn(self.layer(combined))\n",
    "            forget_gate, input_gate, cell_gate, output_gate = gates.chunk(4, dim=1)\n",
    "\n",
    "            \n",
    "            # Apply activation functions\n",
    "            forget_gate = torch.sigmoid(forget_gate)\n",
    "            input_gate = torch.sigmoid(input_gate)\n",
    "            cell_gate = torch.tanh(cell_gate)\n",
    "            output_gate = torch.sigmoid(output_gate)\n",
    "\n",
    "            \n",
    "            # Update states\n",
    "            c = forget_gate * c + input_gate * cell_gate\n",
    "            h = output_gate * torch.tanh(c)\n",
    "\n",
    "            outputs.append(h)\n",
    "\n",
    "        return outputs, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57322e6-d2ed-4855-9529-2d95942427e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, device, num_epochs=25, learning_rate=3e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses, accuracies = [], []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_accuracy = 0, 0\n",
    "        batch_losses, batch_accuracies = [], []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for i, (x_train, y_train) in pbar:\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device).long()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = model(x_train)  # Assuming your LSTM returns output and hidden states\n",
    "            outputs = torch.stack(outputs).mean(dim=0)  # Mean pooling across time dimension\n",
    "            y_pred = outputs[:, -1, :]  # Use the last time step for classification\n",
    "\n",
    "            # Loss calculation\n",
    "            loss = criterion(y_pred, y_train)\n",
    "            batch_losses.append(loss.item())\n",
    "            epoch_loss = np.mean(batch_losses)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_description(f\"Epoch {e+1} Batch {i+1}: loss = {epoch_loss:.5f}\")\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pbar_t = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "            for i, (x_test, y_test) in pbar_t:\n",
    "                x_test = x_test.to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                outputs, _ = model(x_test)\n",
    "                outputs = torch.stack(outputs).mean(dim=0)\n",
    "                y_pred = outputs[:, -1, :]\n",
    "\n",
    "                # Accuracy calculation\n",
    "                accuracy = (torch.argmax(y_pred, dim=1) == y_test).float().mean()\n",
    "                batch_accuracies.append(accuracy.item())\n",
    "                epoch_accuracy = np.mean(batch_accuracies)\n",
    "\n",
    "                pbar_t.set_description(f\"Epoch {e+1} Batch {i+1}: accuracy = {epoch_accuracy:.5f}\")\n",
    "\n",
    "            accuracies.append(epoch_accuracy)\n",
    "\n",
    "    return losses, accuracies, time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f87a6f-89f1-4226-a67b-86c13f9a9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just trying to implementing the LSTM Model.\n",
    "\n",
    "frame_dim = (64, 64)  # Frame dimensions\n",
    "input_size = 32  # Example input size after convolution\n",
    "hidden_size = 128  # Hidden size of LSTM\n",
    "num_channels = 3  # Number of channels in the frame\n",
    "\n",
    "lstm_model = LSTM(input_size=input_size, hidden_size=hidden_size, num_channels=num_channels, frame_dim=frame_dim)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lstm_model = LSTM(input_size=input_size, hidden_size=hidden_size, num_channels=num_channels, frame_dim=frame_dim).to(device)  # Initialize your LSTM model with appropriate parameters\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "losses, accuracies, duration = train(lstm_model, train_loader, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a0950-12d3-448d-9e21-7c8dd4066ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_channels: int, frame_dim: tuple):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size, self.hidden_size = input_size, hidden_size\n",
    "        self.frame_dim = frame_dim\n",
    "\n",
    "        # Convolutional encoder\n",
    "        self.conv_encoder = ConvolutionalEncoder(num_channels, input_size)\n",
    "\n",
    "        # GRU cell\n",
    "        self.gru_cell = nn.GRUCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            frame = x[:, t, :, :, :]  # Select the t-th frame for each sequence\n",
    "            conv_out = self.conv_encoder(frame)\n",
    "\n",
    "            h = self.gru_cell(conv_out, h)\n",
    "            outputs.append(h)\n",
    "\n",
    "        return outputs, h\n",
    "\n",
    "gru_model = GRU(input_size=32, hidden_size=128, num_channels=3, frame_dim=(64,64)).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "losses, accuracies, duration = train(gru_model, train_loader, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcc9fe-82f3-41c2-8c35-4d898a89487f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73033849-72a1-4b41-83f7-0a0db245566d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
